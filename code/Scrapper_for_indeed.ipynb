{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scrapping\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "\n",
    "#Analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as sm\n",
    "from sklearn import preprocessing, linear_model, metrics\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# importing needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_job(row_result):\n",
    "#     try:\n",
    "#         job = row_result.find_all(class_='jobtitle')[0].text.strip()\n",
    "#     except:\n",
    "#         job = None\n",
    "        \n",
    "#     return job\n",
    "\n",
    "# #     This scraps and parses our job\n",
    "# # ('a', attrs={'class' : 'turnstileLink'}).attrs['title'] is where our job title is hidden .text.strip cleanes out the \n",
    "# # html atrifacts. It also reutns non if the jobe title is empty at this location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def extract_salary(row_result):\n",
    "    \n",
    "    \n",
    "# #     '''\n",
    "# #     In this function, I iterate through salary data searching for specific combinations of string structure using regex.\n",
    "# #     '''\n",
    "    \n",
    "#     regex_options = ['\\$[0-9]*.[0-9]* - \\$[0-9]*.[0-9]* [a-zA-Z]* [a-zA-Z]*', \\\n",
    "#                      '\\$[0-9]* - \\$[0-9]* [a-zA-Z]* [a-zA-Z]*', \\\n",
    "#                      '\\$[0-9]*.[0-9]* [a-zA-Z]* [a-zA-Z]*'\\\n",
    "#                     ]\n",
    "\n",
    "#     for regex in regex_options:\n",
    "#         search = re.search(regex, row_result.text, flags=0)\n",
    "#         if search:\n",
    "#             return search.group(0)\n",
    "        \n",
    "#     return None\n",
    "\n",
    "# #     try:\n",
    "# #         return row_result.find('td',{'class' : 'snip'}).find('nobr').text\n",
    "# #     except:\n",
    "# #         return None\n",
    "# # ^ so this was my first salary scrapper\n",
    "# # then I did some reading and talk to some classmates. They helped me figure out this regex that parse out the salary in a format we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def extract_desc(row_result):\n",
    "#     try:\n",
    "#         return row_result.find('span',{'class' : 'summary'}).renderContents\n",
    "#     except:\n",
    "#         return None\n",
    "# # This is where I wish I had done something diffrent. Render contents left some HTml artifacts that were a pain down the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def extract_location(row_result):\n",
    "#     try:\n",
    "#         loc = row_result.find_all(class_='location')[0].text.strip()\n",
    "#     except:\n",
    "#         loc = None\n",
    "        \n",
    "#     return loc\n",
    "# #     x = row_result.find('span', {'class':'location'}).renderContents()\n",
    "# #     if 'itemprop=\"addressLocality' in x:\n",
    "# #         x = i.find('span', {'itemprop' : 'addressLocality'}).renderContents()\n",
    "# #     return x\n",
    "# # again for all of these the first time through I didnt use a except statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def extract_company(row_result):\n",
    "#     try:\n",
    "#         comp = row_result.find_all(class_='company')[0].text.strip()\n",
    "#     except:\n",
    "#         comp = None\n",
    "        \n",
    "#     return comp\n",
    "\n",
    "# #     try:\n",
    "# #         company = row_result.find('span', {'class' : 'compamny'}).text\n",
    "# #         company = company.replace('\\n','')\n",
    "# #         while company[0]==' ':\n",
    "# #             company= company[1:]\n",
    "# #         return company\n",
    "# #     except:\n",
    "# #         return None\n",
    "# # and again no strip and no except on my first try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in results:\n",
    "#     for result in i.find_all('div', class_=['row', 'result']):\n",
    "#         location = extract_location(result)\n",
    "#         company = extract_company(result)\n",
    "#         job = extract_job(result)\n",
    "#         salary = extract_salary(result)\n",
    "#         desc = extract_desc(result)\n",
    "#         frame.append({'location':location, 'company' :company, 'job' :job, 'salary':salary, 'desc':desc})\n",
    "# test scrapper not great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def scrape_page(soup):\n",
    "# #     '''\n",
    "# #     On each page, this function scrapes all the results on the page and returns a list of dictionaries.\n",
    "# #     '''\n",
    "    \n",
    "#     job_list = []\n",
    "\n",
    "#     for tag in soup.find_all(class_='result'):\n",
    "#     #     print(tag.text)\n",
    "#         job_dict = {}\n",
    "\n",
    "#         job_dict['Job Title'] = extract_job(tag)\n",
    "#         job_dict['Company'] = extract_company(tag)\n",
    "#         job_dict['Location'] = extract_location(tag)\n",
    "#         job_dict['Salary'] = extract_salary(tag)\n",
    "#         job_dict['Description'] = extract_desc(tag)\n",
    "                             \n",
    "\n",
    "#         job_list.append(job_dict)\n",
    "\n",
    "#     return job_list\n",
    "# # this is the meat of it. The scrapper function it takes a soup element or the raw html and uses our functions above to parse out\n",
    "# # the data we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# url_template = 'http://www.indeed.com/jobs?'\n",
    "\n",
    "# jobs = ['Data Scientist', 'Data Analyst', 'Data Science', 'Data Analysis','Data Engineer', 'Data Architect']\n",
    "\n",
    "# salaries = [i * 10000 for i in range(1, 501)]\n",
    "\n",
    "# radius = 'radius=100'\n",
    "\n",
    "# starts = [i * 10 for i in range(100)]\n",
    "# # here we define the other elements we will plug into our html for each iteration by job title sallries with a list comp\n",
    "# # start tells the scraper scrap 1000 enterys per the rest of our html inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I created a set of functions that I used to generate various components of job links. These components include -\n",
    "# Links\n",
    "# Salaries\n",
    "# Locations\n",
    "# Starts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKING MY REGION STATE LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# locations_NE = ['New+York', 'Buffalo', 'Rochester', 'Washington,DC', 'Baltimore', 'Providence', 'Philadelphia', 'Hartoford']\n",
    "             \n",
    "# locations_MW = ['Chicago', 'Pittsburgh', 'Detroit', 'Minneapolis', 'St.+Louis', 'Cincinnati', 'Milwaukee', 'Oklahoma+City', 'Kansas+City', 'Cleveland', 'Columbus', 'Indianapolis']\n",
    "             \n",
    "# locations_SE = ['Houston', 'Austin', 'Atlanta', 'Dallas', 'Houston', 'Miami', 'Charlotte', 'Orlando', 'Memphis', 'Raleigh', 'New+Orleans', 'Birmingham', 'Tampa', 'San+Antonio']\n",
    "             \n",
    "# locations_CA = ['San+Francisco', 'Los+Angeles', 'San+Diego', 'Sacramento', 'San+Jose'] \n",
    "             \n",
    "# locations_NW = ['Portland', 'Phoenix', 'Denver', 'Seattle', 'Las+Vegas', 'Salt+Lake+City']\n",
    "    \n",
    "# # #     'Washington D.C.', 'New York', 'San Fransisco', 'Chicago', 'Austin', 'Jacksonville', 'Indianapolis'\\\n",
    "# #              'Seattle', 'Los Angeles', 'Philadelphia', 'Atlanta', 'Dallas', 'Columbus', 'Fort Worth', 'Charlotte' \\\n",
    "# #              'Pittsburgh', 'Portland', 'Phoenix', 'Denver', 'Houston', 'Miami', 'San Antonio', 'San Diego', 'San Jose'\\\n",
    "# #              'Nashville', 'El Paso', 'Boston', 'Detroit', 'Memphis', 'Oklahoma City', 'Baltimore', 'Las Vegas' \\\n",
    "# #              'Milwaukee', 'Albuquerque', 'Tucson', 'Fresno', 'Scramento', 'Mesa', 'Kansas City', 'Raleigh']\n",
    "# #  I ran into a ram issue on one of my first runs so I made my locations into regional lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dont for get to change location converstion list comps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def conv_jobs(jobs):\n",
    "#     return [('q=' + job.lower().replace(' ', '+')) for job in jobs]\n",
    "\n",
    "# def conv_salaries(salaries):\n",
    "#     return [('+$' + str(sal)) for sal in salaries]\n",
    "\n",
    "# def conv_locations(locations_NW):\n",
    "#     return [('&l=' + loc.replace(' ', '+')) for loc in locations_NW]\n",
    "\n",
    "# def conv_starts(starts):\n",
    "#     return ['&start=' + str(start) for start in starts]\n",
    "# Here we have a few functions that formmat our html inputs like city and state so it can easily add a new one for each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dont for get to change DF Name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# columns = \"Job Company Location Salary Description\"\n",
    "# df_NW = pd.DataFrame(columns = columns.split())\n",
    "# now we intilize our empty dF we reset these NW values each time we do a new regiona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # i = 0\n",
    "# for our counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dont for add new location list and new DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 1000 Jobs.13 of these aren't junk.\n",
      "Got 2000 Jobs.26 of these aren't junk.\n",
      "Got 3000 Jobs.26 of these aren't junk.\n",
      "Got 4000 Jobs.35 of these aren't junk.\n",
      "Got 5000 Jobs.56 of these aren't junk.\n",
      "Got 6000 Jobs.70 of these aren't junk.\n",
      "Got 7000 Jobs.72 of these aren't junk.\n",
      "Got 8000 Jobs.88 of these aren't junk.\n",
      "Got 9000 Jobs.90 of these aren't junk.\n",
      "Got 10000 Jobs.90 of these aren't junk.\n",
      "Got 11000 Jobs.90 of these aren't junk.\n",
      "Got 12000 Jobs.91 of these aren't junk.\n",
      "Got 13000 Jobs.91 of these aren't junk.\n",
      "Got 14000 Jobs.94 of these aren't junk.\n",
      "Got 15000 Jobs.94 of these aren't junk.\n",
      "Got 16000 Jobs.94 of these aren't junk.\n",
      "Got 17000 Jobs.94 of these aren't junk.\n",
      "Got 18000 Jobs.94 of these aren't junk.\n",
      "Got 19000 Jobs.94 of these aren't junk.\n",
      "Got 20000 Jobs.94 of these aren't junk.\n",
      "Got 21000 Jobs.94 of these aren't junk.\n",
      "Got 22000 Jobs.94 of these aren't junk.\n",
      "Got 23000 Jobs.97 of these aren't junk.\n",
      "Got 24000 Jobs.97 of these aren't junk.\n",
      "Got 25000 Jobs.97 of these aren't junk.\n",
      "Got 26000 Jobs.97 of these aren't junk.\n",
      "Got 27000 Jobs.97 of these aren't junk.\n",
      "Got 28000 Jobs.97 of these aren't junk.\n",
      "Got 29000 Jobs.97 of these aren't junk.\n",
      "Got 30000 Jobs.98 of these aren't junk.\n",
      "Got 31000 Jobs.98 of these aren't junk.\n",
      "Got 32000 Jobs.98 of these aren't junk.\n",
      "Got 33000 Jobs.98 of these aren't junk.\n",
      "Got 34000 Jobs.98 of these aren't junk.\n",
      "Got 35000 Jobs.98 of these aren't junk.\n",
      "Got 36000 Jobs.98 of these aren't junk.\n",
      "Got 37000 Jobs.98 of these aren't junk.\n",
      "Got 38000 Jobs.98 of these aren't junk.\n",
      "Got 39000 Jobs.98 of these aren't junk.\n",
      "Got 40000 Jobs.98 of these aren't junk.\n",
      "Got 41000 Jobs.98 of these aren't junk.\n",
      "Got 42000 Jobs.98 of these aren't junk.\n",
      "Got 43000 Jobs.98 of these aren't junk.\n",
      "Got 44000 Jobs.98 of these aren't junk.\n",
      "Got 45000 Jobs.98 of these aren't junk.\n",
      "Got 46000 Jobs.98 of these aren't junk.\n",
      "Got 47000 Jobs.98 of these aren't junk.\n",
      "Got 48000 Jobs.98 of these aren't junk.\n",
      "Got 49000 Jobs.99 of these aren't junk.\n",
      "Got 50000 Jobs.99 of these aren't junk.\n",
      "Got 51000 Jobs.99 of these aren't junk.\n",
      "Got 52000 Jobs.99 of these aren't junk.\n",
      "Got 53000 Jobs.99 of these aren't junk.\n",
      "Got 54000 Jobs.99 of these aren't junk.\n",
      "Got 55000 Jobs.99 of these aren't junk.\n",
      "Got 56000 Jobs.99 of these aren't junk.\n",
      "Got 57000 Jobs.99 of these aren't junk.\n",
      "Got 58000 Jobs.99 of these aren't junk.\n",
      "Got 59000 Jobs.99 of these aren't junk.\n",
      "Got 60000 Jobs.99 of these aren't junk.\n",
      "Got 61000 Jobs.99 of these aren't junk.\n",
      "Got 62000 Jobs.99 of these aren't junk.\n",
      "Got 63000 Jobs.99 of these aren't junk.\n",
      "Got 64000 Jobs.99 of these aren't junk.\n",
      "Got 65000 Jobs.99 of these aren't junk.\n",
      "Got 66000 Jobs.101 of these aren't junk.\n",
      "Got 67000 Jobs.101 of these aren't junk.\n",
      "Got 68000 Jobs.101 of these aren't junk.\n",
      "Got 69000 Jobs.101 of these aren't junk.\n",
      "Got 70000 Jobs.101 of these aren't junk.\n",
      "Got 71000 Jobs.102 of these aren't junk.\n",
      "Got 72000 Jobs.102 of these aren't junk.\n",
      "Got 73000 Jobs.102 of these aren't junk.\n",
      "Got 74000 Jobs.103 of these aren't junk.\n",
      "Got 75000 Jobs.103 of these aren't junk.\n",
      "Got 76000 Jobs.103 of these aren't junk.\n",
      "Got 77000 Jobs.103 of these aren't junk.\n",
      "Got 78000 Jobs.103 of these aren't junk.\n",
      "Got 79000 Jobs.103 of these aren't junk.\n",
      "Got 80000 Jobs.103 of these aren't junk.\n",
      "Got 81000 Jobs.103 of these aren't junk.\n",
      "Got 82000 Jobs.103 of these aren't junk.\n",
      "Got 83000 Jobs.103 of these aren't junk.\n",
      "Got 84000 Jobs.103 of these aren't junk.\n",
      "Got 85000 Jobs.103 of these aren't junk.\n",
      "Got 86000 Jobs.103 of these aren't junk.\n",
      "Got 87000 Jobs.103 of these aren't junk.\n",
      "Got 88000 Jobs.103 of these aren't junk.\n",
      "Got 89000 Jobs.104 of these aren't junk.\n",
      "Got 90000 Jobs.105 of these aren't junk.\n",
      "Got 91000 Jobs.105 of these aren't junk.\n",
      "Got 92000 Jobs.105 of these aren't junk.\n",
      "Got 93000 Jobs.105 of these aren't junk.\n",
      "Got 94000 Jobs.105 of these aren't junk.\n",
      "Got 95000 Jobs.105 of these aren't junk.\n",
      "Got 96000 Jobs.105 of these aren't junk.\n",
      "Got 97000 Jobs.105 of these aren't junk.\n",
      "Got 98000 Jobs.105 of these aren't junk.\n",
      "Got 99000 Jobs.105 of these aren't junk.\n",
      "Got 100000 Jobs.105 of these aren't junk.\n",
      "Got 101000 Jobs.105 of these aren't junk.\n",
      "Got 102000 Jobs.105 of these aren't junk.\n",
      "Got 103000 Jobs.105 of these aren't junk.\n",
      "Got 104000 Jobs.105 of these aren't junk.\n",
      "Got 105000 Jobs.105 of these aren't junk.\n",
      "Got 106000 Jobs.105 of these aren't junk.\n",
      "Got 107000 Jobs.105 of these aren't junk.\n",
      "Got 108000 Jobs.105 of these aren't junk.\n",
      "Got 109000 Jobs.105 of these aren't junk.\n",
      "Got 110000 Jobs.105 of these aren't junk.\n",
      "Got 111000 Jobs.105 of these aren't junk.\n",
      "Got 112000 Jobs.105 of these aren't junk.\n",
      "Got 113000 Jobs.105 of these aren't junk.\n",
      "Got 114000 Jobs.105 of these aren't junk.\n",
      "Got 115000 Jobs.105 of these aren't junk.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-948865e1bf45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                    \u001b[0;32mif\u001b[0m \u001b[0mweb_page\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m404\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                        \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweb_page\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                        \u001b[0mjobs_on_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/natalie/anaconda/lib/python2.7/site-packages/bs4/__init__.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mParserRejectedMarkup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/natalie/anaconda/lib/python2.7/site-packages/bs4/__init__.pyc\u001b[0m in \u001b[0;36m_feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/natalie/anaconda/lib/python2.7/site-packages/bs4/builder/_htmlparser.pyc\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTMLParseError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             warnings.warn(RuntimeWarning(\n",
      "\u001b[0;32m/Users/natalie/anaconda/lib/python2.7/HTMLParser.pyc\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \"\"\"\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoahead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/natalie/anaconda/lib/python2.7/HTMLParser.pyc\u001b[0m in \u001b[0;36mgoahead\u001b[0;34m(self, end)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstarttagopen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# < + letter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_starttag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"</\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_endtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/natalie/anaconda/lib/python2.7/HTMLParser.pyc\u001b[0m in \u001b[0;36mparse_starttag\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_startendtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_starttag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDATA_CONTENT_ELEMENTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cdata_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/natalie/anaconda/lib/python2.7/site-packages/bs4/builder/_htmlparser.pyc\u001b[0m in \u001b[0;36mhandle_starttag\u001b[0;34m(self, name, attrs, handle_empty_element)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mattrvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\"\"'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m#print \"START\", name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_starttag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_empty_element\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhandle_empty_element\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m# Unlike other parsers, html.parser doesn't send separate end tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/natalie/anaconda/lib/python2.7/site-packages/bs4/__init__.pyc\u001b[0m in \u001b[0;36mhandle_starttag\u001b[0;34m(self, name, namespace, nsprefix, attrs)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         tag = Tag(self, self.builder, name, namespace, nsprefix, attrs,\n\u001b[0;32m--> 465\u001b[0;31m                   self.currentTag, self._most_recent_element)\n\u001b[0m\u001b[1;32m    466\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/natalie/anaconda/lib/python2.7/site-packages/bs4/element.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parser, builder, name, namespace, prefix, attrs, parent, previous, is_xml)\u001b[0m\n\u001b[1;32m    844\u001b[0m                 \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0;31m# If possible, determine ahead of time whether this tag is an\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#  for job in conv_jobs(jobs):\n",
    "#         for sal in conv_salaries(salaries):\n",
    "#             for loc in conv_locations(locations_NW):\n",
    "#                 for start in conv_starts(starts):\n",
    "                    \n",
    "#                     url = url_template + job + sal + loc + start\n",
    "#                     web_page = requests.get(url)\n",
    "                \n",
    "# #                     if web_page.status_code != 404:\n",
    "#                         soup = BeautifulSoup(web_page.text, 'html.parser')\n",
    "#                         jobs_on_page = scrape_page(soup)\n",
    "                    \n",
    "#                         for a_job in jobs_on_page:\n",
    "# #                             print(a_job)\n",
    "#                             df_NW.loc[i] = {'Job': a_job['Job Title'], 'Company': a_job['Company'], 'Location': a_job['Location'], 'Salary': a_job['Salary'],'Description': a_job['Description']}\n",
    "#                             i += 1\n",
    "#                             if not (i % 1000):\n",
    "#                                 print('Got ' + str(i) + ' Jobs.' + str(df_NW.dropna().drop_duplicates().shape[0]) + \" of these aren't junk.\")\n",
    "#                   now we scrape with our soup element this net counter Ram showed me dose a good job \n",
    "# vizulizing progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# url_temp = \"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l={}&start={}\"\n",
    "# max_per_city = 3000\n",
    "# #crawling more results will take ahile so this is a test will run at 5000 later tonight\n",
    "\n",
    "# results = []\n",
    "# frame = []\n",
    "\n",
    "# for city in set(['New+York', 'Chicago', 'San+Francisco', 'Austin', 'Seattle', \n",
    "#     'Los+Angeles', 'Philadelphia', 'Atlanta', 'Dallas', 'Pittsburgh', \n",
    "#     'Portland', 'Phoenix', 'Denver', 'Houston', 'Miami', 'San+Diego', \n",
    "#     'Washington,DC','Detroit','Minneapolis', 'Tampa', 'St.+Louis', 'Baltimore', \n",
    "#     'Charlotte', 'Orlando', 'San+Antonio', 'Sacramento', 'Cincinnati', 'Las+Vegas', \n",
    "#     'Kansas+City', 'Cleveland', 'Columbus', 'Indianapolis', 'San Jose', 'Providence', 'Milwaukee', \n",
    "#     'Oklahoma+City', 'Memphis', 'Raleigh', 'New+Orleans', 'Hartoford', 'Salt+Lake+City',\n",
    "#     'Birmingham', 'Buffalo', 'Rochester']):\n",
    "#     for start in range(0, max_per_city, 10):\n",
    "#         URL = \"https://www.indeed.com/jobs?as_and=data+scientist&as_phr=&as_any=&as_not=&as_ttl=&as_cmp=&jt=fulltime&st=&sr=directhire&salary=&radius=100&l={}&fromage=any&limit=10&sort=date&psf=advsrch&start={}\".format(city, start)\n",
    "#         r = urllib2.urlopen(URL).read()\n",
    "#         soup = BeautifulSoup(r, 'html.parser', from_encoding='utf-8')\n",
    "#         results.append(soup)\n",
    "#         # Grab the results from the request (as above)\n",
    "#         # Append to the full set of results\n",
    "#         pass\n",
    "# first attempt only worked under 1000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115555, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_NW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRE Research Analyst - Portland</td>\n",
       "      <td>Cushman &amp; Wakefield</td>\n",
       "      <td>Portland, OR 97201</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;bound method Tag.renderContents of &lt;span clas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dir Advanced Analytics</td>\n",
       "      <td>The Standard</td>\n",
       "      <td>Portland, OR 97204</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;bound method Tag.renderContents of &lt;span clas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject Matter Expert/Artificial Intelligence</td>\n",
       "      <td>Staff Finders Technical Inc</td>\n",
       "      <td>Hillsboro, OR</td>\n",
       "      <td>$70 an hour</td>\n",
       "      <td>&lt;bound method Tag.renderContents of &lt;span clas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist I/Data Warehouse Developer</td>\n",
       "      <td>Moda Health</td>\n",
       "      <td>Portland, OR 97204 (Downtown area)</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;bound method Tag.renderContents of &lt;span clas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Consumer Data Scientist</td>\n",
       "      <td>NIKE INC</td>\n",
       "      <td>Beaverton, OR</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;bound method Tag.renderContents of &lt;span clas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Job                      Company  \\\n",
       "0                CRE Research Analyst - Portland          Cushman & Wakefield   \n",
       "1                         Dir Advanced Analytics                 The Standard   \n",
       "2  Subject Matter Expert/Artificial Intelligence  Staff Finders Technical Inc   \n",
       "3      Data Scientist I/Data Warehouse Developer                  Moda Health   \n",
       "4                        Consumer Data Scientist                     NIKE INC   \n",
       "\n",
       "                             Location       Salary  \\\n",
       "0                  Portland, OR 97201         None   \n",
       "1                  Portland, OR 97204         None   \n",
       "2                       Hillsboro, OR  $70 an hour   \n",
       "3  Portland, OR 97204 (Downtown area)         None   \n",
       "4                       Beaverton, OR         None   \n",
       "\n",
       "                                         Description  \n",
       "0  <bound method Tag.renderContents of <span clas...  \n",
       "1  <bound method Tag.renderContents of <span clas...  \n",
       "2  <bound method Tag.renderContents of <span clas...  \n",
       "3  <bound method Tag.renderContents of <span clas...  \n",
       "4  <bound method Tag.renderContents of <span clas...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_NW.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_NW.to_csv('df-NW.csv', encoding='utf-8')\n",
    "# after scrapping each region we save them to a CSV and then move over to data cleaning notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 51K is where it crapped out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_NE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.Location.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df.location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
